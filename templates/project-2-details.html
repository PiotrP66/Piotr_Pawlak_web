{% include "header.html" %}

  <main class="main">

    <!-- Page Title -->
    <div class="page-title" data-aos="fade">
      <div class="container">
        <nav class="breadcrumbs">
          <ol>
            <li><a href="/">Strona startowa</a></li>
            <li class="current">Web scraping</li>
          </ol>
        </nav>
        <h1>Szczególy projektu</h1>
      </div>
    </div><!-- End Page Title -->

    <!-- Portfolio Details Section -->
    <section id="portfolio-details" class="portfolio-details section">

      <div class="container" data-aos="fade-up">

        <div class="row justify-content-between gy-4 mt-4">

          <div class="col-lg-12" data-aos="fade-up" data-aos-delay="100">
            <div class="portfolio-description">
              
              <div class="container" data-aos="fade-up" data-aos-delay="100">
                <div class="row gy-4">
                  <h1>
                    Web Scraping: Automatyczne zbieranie danych z ogłoszeń mieszkań na sprzedaż z serwisu Otodom.pl
                  </h1>
                </div>
              </div>

              
              <div class="container py-4" data-aos="fade-up" data-aos-delay="100">
                  </h2>
                  <p>
                    Projekt ma na celu automatyczne pozyskiwanie danych z serwisu Otodom.pl, agregującego ogłoszenia sprzedaży mieszkań. Zgromadzone dane z rynku pierwotnego i wtórnego Warszawy posłużą do dalszej analizy trendów cenowych, preferencji kupujących oraz charakterystyki dostępnych nieruchomości.
                  </p>
              </div>

              <div class="col-lg-3" data-aos="fade-up" data-aos-delay="100">
                <div class="portfolio-description">
                  <h2>
                    <a href="https://www.otodom.pl/">
                      <img src="static/assets/img/portfolio/otodom_logo_primary.png" class="img-fluid" alt="otodom logo">
                    </a>
                </div>
              </div>

              <div class="portfolio-description">
                <div class="portfolio-details-slider swiper init-swiper">
                  <script type="application/json" class="swiper-config">
                    {
                      "loop": true,
                      "speed": 600,
                      "autoplay": {
                        "delay": 5000
                      },
                      "slidesPerView": "auto",
                      "navigation": {
                        "nextEl": ".swiper-button-next",
                        "prevEl": ".swiper-button-prev"
                      },
                      "pagination": {
                        "el": ".swiper-pagination",
                        "type": "bullets",
                        "clickable": true
                      }
                    }
                  </script>
                  <div class="swiper-wrapper align-items-center">
        
                    <div class="swiper-slide">
                      <img src="static/assets/img/portfolio/otodom_1.jpg" alt="">
                    </div>
        
                    <div class="swiper-slide">
                      <img src="static/assets/img/portfolio/otodom_2.jpg" alt="">
                    </div>
        
                    <div class="swiper-slide">
                      <img src="static/assets/img/portfolio/otodom_3.jpg" alt="">
                    </div>
        
                  </div>
                  <div class="swiper-button-prev"></div>
                  <div class="swiper-button-next"></div>
                  <div class="swiper-pagination"></div>
                </div>
              </div>

              <!-- Services Section -->
              <section id="services" class="services section">

                <!-- Section Title -->
                <div class="container section-title py-1" data-aos="fade-up">
                  <h2>Technologie i biblioteki</h2>
                </div><!-- End Section Title -->

                <div class="container">

                  <div class="row gy-4">

                    <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="100">
                      <div class="service-item item-cyan position-relative">
                        <div class="icon">
                          <img src="static/assets/img/portfolio/python_icon.svg" class="img-fluid"></img>
                        </div>
                        <a href="https://www.python.org/" class="stretched-link">
                          <h3>Python</h3>
                        </a>
                        <p>
                          Język programowania wykorzystany do implementacji skryptu.
                        </p>
                      </div>
                    </div><!-- End Service Item -->

                    <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="200">
                      <div class="service-item position-relative">
                        <div class="icon">
                          <img src="static/assets/img/portfolio/selenium_icon.svg" class="img-fluid"></img>
                        </div>
                        <a href="https://www.selenium.dev/" class="stretched-link">
                          <h3>Selenium WebDriver</h3>
                        </a>
                        <p>
                          Automatyzacja przeglądarki internetowej, symulacja interakcji użytkownika.
                        </p>
                      </div>
                    </div><!-- End Service Item -->

                    <div class="col-lg-4 col-md-6" data-aos="fade-up" data-aos-delay="300">
                      <div class="service-item item-teal position-relative">
                        <div class="icon">
                          <img src="static/assets/img/portfolio/pandas_icon.svg" class="img-fluid"></img>
                        </div>
                        <a href="https://pandas.pydata.org/" class="stretched-link">
                          <h3>Pandas</h3>
                        </a>
                        <p>
                          Biblioteka do analizy i manipulacji danymi, formatowanie i zapis danych do CSV.
                        </p>
                      </div>
                    </div><!-- End Service Item -->

                  </div>

                </div>

              </section><!-- /Services Section -->

              <div class="container section-title py-1" data-aos="fade-up" data-aos-delay="100">
                <h2>
                  Opis programu
                </h2>
              </div>

              <ul>
                <div class="container" data-aos="fade-up" data-aos-delay="100">
                  <li>
                    <h4>Inicjalizacja przeglądarki</h4>
                    <p>
                      Wykorzystując bibliotekę Selenium, skrypt uruchamia przeglądarkę Chrome w tle, co pozwala na interakcję ze stroną internetową.
                    </p>
                  </li>
                </div>
                <div class="container" data-aos="fade-up" data-aos-delay="100">
                  <li>
                    <h4>Pobranie listy URL-i ogłoszeń</h4>
                    <p>
                      Program przechodzi przez kolejne strony wyników wyszukiwania dla mieszkań spełniających określone kryteria (5-pokojowe, Warszawa, do 1.1 mln PLN). Z każdej strony zbierane są unikalne linki do poszczególnych ogłoszeń.
                    </p>
                  </li>
                </div>
                <div class="container" data-aos="fade-up" data-aos-delay="100">
                  <li>
                    <h4>Zapis URL-i</h4>
                    <p>
                      Zebrane adresy URL są tymczasowo zapisywane do pliku CSV, co stanowi pierwszy etap pozyskiwania danych.
                    </p>
                  </li>
                </div>
                <div class="container" data-aos="fade-up" data-aos-delay="100">
                  <li>
                    <h4>Scrapowanie danych z poszczególnych ogłoszeń</h4>
                    <p>
                      Następnie program iteruje po zapisanych URL-ach. Dla każdego URL-a otwiera nową kartę w przeglądarce i za pomocą Selenium lokalizuje oraz ekstrahuje kluczowe informacje z ogłoszenia, takie jak cena, metraż, lokalizacja, liczba pokoi, informacje dodatkowe i dane o sprzedawcy.
                    </p>
                  </li>
                </div>
                <div class="container" data-aos="fade-up" data-aos-delay="100">
                  <li>
                    <h4>Przetwarzanie i zapis danych</h4>
                    <p>
                      Zebrane dane z każdego ogłoszenia są strukturyzowane przy użyciu biblioteki Pandas i dodawane do DataFrame. W celu zabezpieczenia przed utratą danych, częściowy zapis do pliku CSV następuje co kilka przetworzonych ogłoszeń.
                    </p>
                  </li>
                </div>
                <div class="container" data-aos="fade-up" data-aos-delay="100">
                  <li>
                    <h4>Finalny zapis danych</h4>
                    <p>
                      Po przetworzeniu wszystkich URL-i, kompletne dane są zapisywane do pliku CSV z aktualną datą w nazwie, co ułatwia śledzenie i organizację zgromadzonych informacji.
                    </p>
                  </li>
                </div>
                <div class="container" data-aos="fade-up" data-aos-delay="100">
                  <li>
                    <h4>Obsługa błędów</h4>
                    <p>
                      Program zawiera mechanizmy obsługi błędów, informujące o ewentualnych problemach podczas scrapowania danych z poszczególnych ogłoszeń.
                    </p>
                  </li>
                </div>
              </ul>

              <div class="container section-title py-2" data-aos="fade-up" data-aos-delay="100">
                <h2>
                  <i class="fa-brands fa-python"></i>
                  Kod źródłowy - main.py
                </h2>
                <div class="container" data-aos="fade-up" data-aos-delay="100">
                  <div class="portfolio-info">
                    <a href="https://github.com/PiotrP66/Oto-dom-web-scraper" class="btn-visit align-self-start">Github</a>
                  </div>
                </div>
              </div>

              <div class="container" data-aos="fade-up" data-aos-delay="100">
                <div class="language-python">
                  <!-- Kod Python -->
                  <pre>
                  import utils as ut
                  import pandas as pd


                  def main():

                      OTODOM_URL = "https://www.otodom.pl/pl/wyniki/sprzedaz/mieszkanie/mazowieckie/warszawa/warszawa/warszawa?limit=72&by=LATEST&direction=ASC"

                      driver = ut.initialize_webdriver()
                      ut.load_url(driver, OTODOM_URL)

                      last_page_number = ut.find_last_page(driver)
                      if last_page_number is None:
                          last_page_number = 1
                      print(f"Number of pages: {last_page_number}")

                      all_url = pd.DataFrame(columns=["URL"])

                      for current_page in range(1, last_page_number + 1):
                          print("Current page:", current_page)
                          page_url = f"{OTODOM_URL}&page={current_page}"
                          driver.execute_script("window.open('');")
                          driver.switch_to.window(driver.window_handles[1])
                          ut.load_url(driver, page_url)

                          page_url = ut.collect_url(driver)

                          all_url = pd.concat([all_url, page_url], ignore_index=True)

                          print("Collected URLs:", len(all_url.URL))
                          driver.close()
                          driver.switch_to.window(driver.window_handles[0])

                          current_page += 1

                      ut.save_data_to_csv(all_url, "otodom_url.csv")

                      # SCRAPE LISTINGS
                      otodom_data_df = pd.DataFrame(columns=["URL",
                                                            "ID",
                                                            "Title",
                                                            "Price_PLN",
                                                            "Square_meter",
                                                            "Price_sq_meter",
                                                            "Rooms",
                                                            "Street",
                                                            "Subdistrict",
                                                            "District",
                                                            "City",
                                                            "Province",
                                                            "Heating",
                                                            "Floor",
                                                            "Rent_PLN",
                                                            "Condition",
                                                            "Market",
                                                            "Ownership",
                                                            "Available_from",
                                                            "Advertiser_type",
                                                            "Seller"])

                      url_data = pd.read_csv("data/otodom_url.csv")
                      all_url_len = len(url_data.URL)

                      errors = 0
                      current_advert = 0

                      for url in url_data.URL:
                          print(f"Progress: {current_advert + 1} of {all_url_len}")
                          current_advert += 1

                          driver.execute_script("window.open('');")
                          driver.switch_to.window(driver.window_handles[1])
                          ut.load_url(driver, url)

                          new_data = ut.collect_data(driver)
                          if new_data:
                              otodom_data_df.loc[len(otodom_data_df)] = new_data
                              driver.close()
                              driver.switch_to.window(driver.window_handles[0])

                              if current_advert % 10 == 0:
                                  ut.save_data_to_csv(otodom_data_df, "otodom_data.csv")
                              else:
                                  continue
                          else:
                              errors += 1
                              print("Error!")
                              driver.close()
                              driver.switch_to.window(driver.window_handles[0])

                      driver.close()

                      ut.save_data_to_csv(otodom_data_df, "otodom_data.csv")

                      print("Done!")
                      print("Errors:", errors)


                  if __name__ == '__main__':
                      main()
                  </pre>
                </div>
              </div>

              <div class="container section-title py-1" data-aos="fade-up" data-aos-delay="100">
                <h2>
                  <i class="fa-brands fa-python"></i>
                  Kod źródłowy - utils.py
                </h2>
                <div class="container" data-aos="fade-up" data-aos-delay="100">
                  <div class="portfolio-info">
                    <a href="https://github.com/PiotrP66/Oto-dom-web-scraper" class="btn-visit align-self-start">Github</a>
                  </div>
                </div>
              </div>

              <div class="container" data-aos="fade-up" data-aos-delay="100">
                <div class="language-python">
                  <!-- Kod Python -->
                  <pre>
                  Fragment kodu:
                  
                  def collect_url(driver):
                  """Collect URLs from a web page's listings using a Selenium WebDriver.

                  This function searches for all listing elements on the current page
                  (identified by the XPath "//ul//li") and attempts to extract URLs from
                  anchor tags that contain '/pl/oferta/' in their 'href' attribute. Each
                  unique URL is stored in a Pandas DataFrame under the column "URL".
                  Listings without a valid link are skipped.

                  Parameters:
                      driver (selenium.webdriver): An instance of Selenium WebDriver used
                                                  to interact with the web page.

                  Returns:
                      pandas.DataFrame: A DataFrame containing a single column "URL" with
                                        the unique URLs collected from the page.
                  """
                  df = pd.DataFrame(columns=["URL"])
                  # Download all listings:
                  listings = driver.find_elements(
                      By.XPATH, "//ul//li")

                  # Extract url from every listing:
                  for listing in listings:
                      try:
                          url_element = listing.find_element(
                              By.XPATH, ".//a[contains(@href, '/pl/oferta/')]")
                          url = url_element.get_attribute("href")
                          if url in df.URL.values:
                              continue
                          else:
                              df.loc[len(df)] = [url]
                      except NoSuchElementException:
                          continue
                  return df
                  </pre>
                </div>                
              </div>

              <div class="container section-title py-1" data-aos="fade-up" data-aos-delay="100">
                <h2>Podsumowanie</h2>
              </div>

              <div class="container py-4" data-aos="fade-up" data-aos-delay="100">
                <p>
                  Zaprezentowany projekt demonstruje moje umiejętności w automatycznym pozyskiwaniu danych z serwisu Otodom.pl przy użyciu Pythona, Selenium i Pandas. Skrypt efektywnie zbiera szczegółowe informacje o ofertach sprzedaży mieszkań, automatyzując czasochłonny proces manualnego przeglądania. Projekt prezentuje kompetencje w web scrapingu (praca z Selenium) oraz przetwarzaniu i strukturyzowaniu danych (wykorzystanie Pandas do zapisu do CSV). Potencjalne dalsze kroki obejmują rozszerzenie o inne serwisy i zaawansowaną analizę.
                </p>
              </div>            

            <div class="testimonial-item">
              <div>
                <img src="static/assets/img/testimonials/testimonials-2.jpg" class="testimonial-img" alt="">
                <h3>Piotr Pawlak</h3>
                <h4>Autor</h4>
              </div>
            </div>
          </div>

        </div>

      </div>

    </section><!-- /Portfolio Details Section -->

  </main>

  {% include "footer.html" %}